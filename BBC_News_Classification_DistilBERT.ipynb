{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sEBD4Vg5fuu"
      },
      "outputs": [],
      "source": [
        "# Install Libraries\n",
        "\n",
        "!pip install transformers datasets --quiet\n",
        "!pip install -U transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import all necessary libraries\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from datasets import ClassLabel\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "76-NVVTS5oli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Upload the dataset\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "jaCtDe3a5uwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the dataset\n",
        "\n",
        "df = pd.read_csv(\"bbc_news_text_complexity_summarization.csv\", delimiter=',')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "fuZwd7gy509H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert to HuggingFace Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset"
      ],
      "metadata": {
        "id": "EpTdJzY052yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Encode label strings\n",
        "# Create a ClassLabel object with all label names\n",
        "\n",
        "class_label = ClassLabel(names=df[\"labels\"].unique().tolist())\n",
        "\n",
        "def encode_labels(example):\n",
        "    example[\"labels\"] = class_label.str2int(example[\"labels\"])\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(encode_labels)\n"
      ],
      "metadata": {
        "id": "nWykfi1x54UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create the tokenizer\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "-GdyD925552e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text\n",
        "\n",
        "#Tokenization includes: lowercasing, splitting into wordpieces, adding attention masks, and truncation/padding to max length\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "33qwCD8z59aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train-Test Split\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_ds = dataset[\"train\"]\n",
        "test_ds = dataset[\"test\"]\n"
      ],
      "metadata": {
        "id": "J7NHbi745_MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Set tensor format\n",
        "\n",
        "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
      ],
      "metadata": {
        "id": "buxZmjue6A6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define DistilBERT classification model\n",
        "\n",
        "num_classes = len(class_label.names)\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=num_classes\n",
        ")\n"
      ],
      "metadata": {
        "id": "3RtPNA3C6CZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=15,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",           # disable W&B\n",
        ")\n"
      ],
      "metadata": {
        "id": "zssHOrk36EFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute accuracy, precision, recall, and F1 (macro) for model predictions\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average='macro'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision_macro\": precision,\n",
        "        \"recall_macro\": recall,\n",
        "        \"f1_macro\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "EKG4RE1w6GVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer object\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "id": "B5god6IW6Hp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "FiJJ8pBX6Jb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "D-_j0eQL6LFk",
        "outputId": "0bfe627b-bb47-4912-bba7-00e38772d47f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [27/27 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.14586851000785828, 'eval_accuracy': 0.971830985915493, 'eval_precision_macro': 0.9718320079539117, 'eval_recall_macro': 0.9704769868229496, 'eval_f1_macro': 0.9709956723738122, 'eval_runtime': 2.0238, 'eval_samples_per_second': 210.491, 'eval_steps_per_second': 13.341, 'epoch': 15.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Predict on test samples\n",
        "\n",
        "preds_output = trainer.predict(test_ds)       # Predict on test set\n",
        "y_true = preds_output.label_ids               # True labels\n",
        "y_pred = np.argmax(preds_output.predictions, axis=1)  # Predicted labels\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Macro Precision, Recall, F1\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (macro): {precision:.4f}\")\n",
        "print(f\"Recall (macro): {recall:.4f}\")\n",
        "print(f\"F1 (macro): {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "YuC6mexs6Mxo",
        "outputId": "762b9b27-1039-493d-f89f-1efb62929a14"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9718\n",
            "Precision (macro): 0.9718\n",
            "Recall (macro): 0.9705\n",
            "F1 (macro): 0.9710\n"
          ]
        }
      ]
    }
  ]
}